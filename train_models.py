import os

import joblib
import numpy as np
import pandas as pd

# KonfigÃ¼rasyonlar
import config
from configs import banking as config_banking

# AraÃ§lar
from utils.data_loader import DataLoader
from utils.feature_engineering import FeatureEngineer
from models.ranking_model import RankingModel

def ensure_model_dir():
    if not os.path.exists("models/saved"):
        os.makedirs("models/saved")

def train_global_ranker():
    print(f"\n{'='*50}")
    print(f"EÄžÄ°TÄ°M BAÅžLIYOR: GLOBAL DAILY RANKER")
    print(f"Timeframe: {config.TIMEFRAME}")
    print(f"Strict Mode: Veri kesim tarihi {config.TRAIN_END_DATE}")
    print(f"{'='*50}")

    all_data_frames = []
    loader = DataLoader(start_date=config.START_DATE)
    
    # TÃ¼m Tickerlar (config.TICKERS - A1 Core)
    tickers = config.TICKERS
    
    for ticker in tickers:
        print(f"  Veri Ä°ÅŸleniyor: {ticker}...")
        raw_data = loader.get_combined_data(ticker)
        
        if raw_data is None or len(raw_data) < 100:
            print(f"  [UYARI] Yetersiz veri: {ticker}")
            continue
            
        # Feature Engineering (Daily Logic will apply due to config change)
        fe = FeatureEngineer(raw_data)
        features_df = fe.process_all(ticker=ticker)
        
        # Add Ticker Column (Multi-Index iÃ§in gerekli olabilir ama RankingModel level='Date' kullanÄ±yor)
        features_df['Ticker'] = ticker
        
        # Validation Split (Tarihsel)
        if hasattr(config, 'TRAIN_END_DATE') and config.TRAIN_END_DATE:
            mask = features_df.index < config.TRAIN_END_DATE
            features_df = features_df[mask]
        
        all_data_frames.append(features_df)
        
    if not all_data_frames:
        print(f"âŒ HiÃ§ veri bulunamadÄ±.")
        return
        
    # Combine All
    print("  Veriler birleÅŸtiriliyor...")
    full_data = pd.concat(all_data_frames)
    
    # Multi-Index (Date, Ticker) set et
    full_data.reset_index(inplace=True)
    full_data.set_index(['Date', 'Ticker'], inplace=True)
    full_data.sort_index(inplace=True) 
    
    print(f"  Toplam EÄŸitim Verisi: {len(full_data)} satÄ±r.")
    
    ensure_model_dir()
    
    # ---------------------------------------------------------
    # 0. MAKRO VERÄ° (TÃ¼m tickerlar iÃ§in ortak)
    # ---------------------------------------------------------
    # YENÄ°: Makro verileri Ã§ekip feature dataframe'lerine merge edeceÄŸiz
    from utils.macro_data_loader import TurkeyMacroData
    print("  > Makro Veriler Ã‡ekiliyor...")
    macro_loader = TurkeyMacroData()
    macro_df = macro_loader.fetch_all() # Index: Date
    
    # ---------------------------------------------------------
    # 1. LIGHTGBM (RANKING) EÄžÄ°TÄ°MÄ°
    # ---------------------------------------------------------
    print(f"  > Ranking Model (LightGBM) EÄŸitiliyor...")
    
    # Config modÃ¼lÃ¼ olarak banking veriyoruz (Generic bir config yeterli)
    model = RankingModel(full_data, config_banking) 
    
    # Train-Validation Split (Son %10 validation)
    dates = full_data.index.get_level_values('Date').unique()
    split_idx = int(len(dates) * 0.9)
    test_start_date = dates[split_idx]
    
    print(f"  > Validasyon BaÅŸlangÄ±Ã§: {test_start_date}")
    
    # Dataframe split
    train_mask = full_data.index.get_level_values('Date') < test_start_date
    valid_mask = full_data.index.get_level_values('Date') >= test_start_date
    
    df_train = full_data[train_mask]
    df_valid = full_data[valid_mask]
    
    # Instantiate with Train
    ranker = RankingModel(df_train, config_banking)
    
    # Check for optimized params from Optuna (file) first, then config fallback
    custom_params = None
    opt_path = "models/saved/optimized_lgbm_params.joblib"
    if os.path.exists(opt_path):
        custom_params = joblib.load(opt_path)
        print(f"  > Optuna ile bulunan hiperparametreler kullanÄ±lÄ±yor: {opt_path}")
    else:
        cfg_params = getattr(config, 'OPTIMIZED_MODEL_PARAMS', None)
        if cfg_params:
            custom_params = cfg_params
            print(f"  > Config iÃ§indeki OPTIMIZED_MODEL_PARAMS kullanÄ±lÄ±yor: {cfg_params}")
    
    ranker.train(valid_df=df_valid, custom_params=custom_params)
    ranker.save(f"models/saved/global_ranker.pkl")
    print(f"âœ… Global Ranker (LightGBM) EÄŸitimi TamamlandÄ±.")

    # ---------------------------------------------------------
    # 2. TFT (TRANSFORMER) EÄžÄ°TÄ°MÄ°
    # ---------------------------------------------------------
    try:
        print(f"\n  > TFT (Temporal Fusion Transformer) EÄŸitimi BaÅŸlÄ±yor...")
        from models.transformer_model import BIST30TransformerModel
        from utils.feature_engineering import prepare_tft_dataset
        
        # TFT feature'larÄ± zaten FeatureEngineer iÃ§inde eklendi (process_all -> add_transformer_features)
        # Sadece Macro verilerin merge edildiÄŸinden emin olmalÄ±yÄ±z.
        # process_all iÃ§inde makro kullanÄ±lmadÄ±ysa burada merge edebiliriz ama FE iÃ§inde halledildi varsayalÄ±m.
        # FE iÃ§inde macro_loader kullanÄ±lmadÄ±, o zaman burada merge edilmesi lazÄ±m.
        
        # Merge Macro Data Logic (EÄŸer FE iÃ§inde yapÄ±lmadÄ±ysa)
        # Note: full_data has MultiIndex (Date, Ticker) and macro_df has Index (Date)
        
        # Reset Index for Merge
        tft_data = full_data.reset_index()
        
        if not macro_df.empty:
            # Date format check
            # macro_df index is datetime
            # tft_data['Date'] is datetime
            tft_data = pd.merge(tft_data, macro_df, left_on='Date', right_index=True, how='left')
            tft_data.fillna(method='ffill', inplace=True) # Fill macro gaps
            tft_data.fillna(0, inplace=True)
            print("  > Makro veriler TFT datasetine eklendi.")
            
        # FIX: PyTorch Forecasting sÃ¼tun isimlerinde '.' sevmez
        print("  > SÃ¼tun isimleri temizleniyor (PyTorch uyumluluÄŸu iÃ§in)...")
        tft_data.columns = tft_data.columns.str.replace(".", "_", regex=False)

        # Dataset Config
        tft_config_dict = prepare_tft_dataset(tft_data, lookback=60)
        
        # Model Init
        tft_model_wrapper = BIST30TransformerModel(config_banking)
        
        # Split Data (Validasyon ayrÄ±mÄ±)
        cutoff_idx = int(len(tft_data['Date'].unique()) * 0.9)
        cutoff_date = sorted(tft_data['Date'].unique())[cutoff_idx]
        
        train_tft = tft_data[tft_data['Date'] < cutoff_date]
        val_tft = tft_data[tft_data['Date'] >= cutoff_date]
        
        # Create PyTorch Forecasting Datasets
        # Mode='train' means we cutoff last prediction length points
        train_ds = tft_model_wrapper.create_dataset(train_tft, tft_config_dict, mode='train')
        val_ds = tft_model_wrapper.create_dataset(val_tft, tft_config_dict, mode='val') # Val dataset uses exact time range
        
        # Build Model Structure
        tft_model_wrapper.build_model(train_ds)

        # Train
        print(f"  > TFT EÄŸitiliyor (Epochs=30, CPU - Full Mode)...")
        tft_model_wrapper.train(train_ds, val_ds, epochs=30, batch_size=64) # Batch size 64 for speed on CPU
        

        
        # --- SANITY CHECK (DENEY A) ---
        print("\nðŸ”Ž SANITY CHECK: TFT Tahmin VaryansÄ± Kontrol Ediliyor...")
        try:
            # Validation seti Ã¼zerinde tahmin al
            raw_predictions = tft_model_wrapper.model.predict(val_ds.to_dataloader(train=False, batch_size=16), mode="prediction")
            
            # Tensor to numpy
            if hasattr(raw_predictions, 'cpu'):
                preds_np = raw_predictions.cpu().numpy()
            else:
                preds_np = np.array(raw_predictions)
                
            # Flatten if needed (predictions might be [Batch, Prediction Horizon])
            preds_flat = preds_np.flatten()
            
            p_mean = np.mean(preds_flat)
            p_std = np.std(preds_flat)
            p_min = np.min(preds_flat)
            p_max = np.max(preds_flat)
            
            print(f"  ðŸ“Š Ä°statistikler:")
            print(f"  Mean: {p_mean:.6f}")
            print(f"  Std Dev: {p_std:.8f} (Varyans: {p_std**2:.8f})")
            print(f"  Min/Max: {p_min:.6f} / {p_max:.6f}")
            
            if p_std < 1e-5:
                print("  âŒ UYARI: MODEL COLLAPSE! Varyans Ã§ok dÃ¼ÅŸÃ¼k. Model Ã¶ÄŸrenmiyor (hepsi aynÄ± deÄŸeri tahmin ediyor).")
            else:
                print("  âœ… Varyans makul gÃ¶rÃ¼nÃ¼yor. Model farklÄ±laÅŸma Ã¼retiyor.")
                
        except Exception as e:
            print(f"  âš ï¸ Sanity Check sÄ±rasÄ±nda hata: {e}")
        # ------------------------------
        
        # Save dataset parameters for inference (IMPORTANT)
        # We need to save tft_config_dict and dataset params
        import joblib
        joblib.dump(tft_config_dict, "models/saved/tft_config.joblib")
        # joblib.dump(train_ds.get_parameters(), "models/saved/tft_dataset_params.joblib") # Complex object
        
        print(f"âœ… TFT Modeli EÄŸitimi TamamlandÄ±.")
        
    except Exception as e:
        print(f"âŒ TFT EÄŸitimi sÄ±rasÄ±nda hata: {e}")
        import traceback
        traceback.print_exc()

def main():
    train_global_ranker()

if __name__ == "__main__":
    main()
