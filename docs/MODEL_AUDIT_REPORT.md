# BIST30 AI Trader - KapsamlÄ± Model DeÄŸerlendirme ve Ä°yileÅŸtirme Raporu

## 1. YÃ–NETÄ°CÄ° Ã–ZETÄ° (Executive Summary)

BIST30 AI Trader projesi, modern finansal yapay zeka yaklaÅŸÄ±mlarÄ±nÄ± (Ranking Learning, Time-Series Forecasting) klasik algoritmik trading prensipleriyle (Risk YÃ¶netimi, PortfÃ¶y Optimazasyonu) birleÅŸtiren hibrit ve gÃ¼Ã§lÃ¼ bir mimariye sahiptir. 

**Mevcut Durum:**
- **Model:** LightGBM (LambdaRank) ve Temporal Fusion Transformer (TFT) olmak Ã¼zere iki ana model kullanÄ±lmaktadÄ±r.
- **Veri:** Yahoo Finance ve EVDS (TCMB) kaynaklÄ± Ã§oklu veri akÄ±ÅŸÄ± mevcuttur.
- **AltyapÄ±:** Linux/ROCm Ã¼zerinde GPU destekli eÄŸitim ortamÄ± baÅŸarÄ±yla kurulmuÅŸtur.
- **Risk:** ATR tabanlÄ± dinamik stop-loss ve piyasa rejimi (Regime Detection) filtreleri mevcuttur.

**Kritik Bulgular:**
1.  **Hiperparametre Optimizasyonu EksikliÄŸi:** Kod tabanÄ±nda `optimized_lgbm_params.joblib` dosyasÄ± aranÄ±yor olsa da, bu dosyayÄ± Ã¼reten ve dÃ¼zenli optimize eden bir script bulunamadÄ±.
2.  **Validasyon Stratejisi:** Mevcut `train_models.py` sadece son %10'luk dilimi validasyon iÃ§in ayÄ±rÄ±yor. Finansal zaman serileri iÃ§in "Walk-Forward Validation" daha gÃ¼venilirdir.
3.  **Backtest GerÃ§ekÃ§iliÄŸi:** Backtest motoru (`core/backtesting.py`) oldukÃ§a geliÅŸmiÅŸ (slippage, market impact var), ancak stres testleri (Stress Testing) eksik.
4.  **Feature Selection:** Feature sayÄ±sÄ± oldukÃ§a fazla. `RankingModel` iÃ§inde basit bir SHAP analizi var ancak sistematik bir eleme (RFE veya Null Importance) yok.

**Ã–ngÃ¶rÃ¼len Ä°yileÅŸme:**
AÅŸaÄŸÄ±daki yol haritasÄ± uygulandÄ±ÄŸÄ±nda, Sharpe OranÄ±'nÄ±n **+0.5 ile +1.0** arasÄ±nda artmasÄ± ve Max Drawdown'Ä±n **%5-10** oranÄ±nda azalmasÄ± hedeflenmektedir.

---

## 2. DETAYLI PROJE ANALÄ°ZÄ°

### A. Genel Mimari
Proje modÃ¼ler bir yapÄ±ya sahiptir:
- `core/`: Ã‡ekirdek motorlar (Backtest, Risk, Veri)
- `models/`: Model tanÄ±mlarÄ± (LightGBM, TFT)
- `utils/`: YardÄ±mcÄ± araÃ§lar (Feature Engineering, Data Loader)

**DeÄŸerlendirme:**
âœ… **GÃ¼Ã§lÃ¼ YÃ¶nler:**
- **Event-Driven Backtest:** `Backtester` sÄ±nÄ±fÄ±, vektÃ¶rel deÄŸil olay tabanlÄ± Ã§alÄ±ÅŸarak (loop over rows) gerÃ§ek hayatÄ± daha iyi simÃ¼le ediyor.
- **Hibrit Model:** Hem sÄ±ralama (LGBM) hem zaman serisi (TFT) modellerinin bir arada dÃ¼ÅŸÃ¼nÃ¼lmesi vizyoner bir yaklaÅŸÄ±m.
- **Risk KatmanÄ±:** Modelden baÄŸÄ±msÄ±z Ã§alÄ±ÅŸan `RiskManager` sÄ±nÄ±fÄ±, "Stop Loss" ve "Trailing Stop" mekanizmalarÄ±nÄ± merkezi yÃ¶netiyor.

âŒ **ZayÄ±f YÃ¶nler:**
- **Feature Store:** Veriler anlÄ±k hesaplanÄ±yor (`FeatureEngineer`). BÃ¼yÃ¼k Ã¶lÃ§ekte bu yavaÅŸlÄ±ÄŸa neden olabilir. Bir feature store (Ã¶rn: Parquet tabanlÄ±) tam oturmamÄ±ÅŸ.
- **Config BaÄŸÄ±mlÄ±lÄ±ÄŸÄ±:** BirÃ§ok kritik eÅŸik deÄŸer (`config.py` iÃ§inde) hardcoded durumda. BunlarÄ±n optimize edilmesi gerekiyor.

### B. Model Analizi

#### 1. LightGBM (Ranking Model)
- **Tip:** `lambdarank` (Learning to Rank)
- **Hedef:** `Excess_Return` (BIST100'e gÃ¶re getiri farkÄ±) sÄ±ralamasÄ±.
- **Durum:** LambdaRank kullanÄ±mÄ± harika bir tercih. Borsa, "ne kadar artacak"tan ziyade "hangisi diÄŸerinden daha iyi artacak" problemidir.
- **Eksik:** Hiperparametreler (learning_rate=0.03, num_leaves=64) *statik*. Her piyasa dÃ¶ngÃ¼sÃ¼ iÃ§in bu deÄŸerler optimal olmayabilir.

#### 2. Temporal Fusion Transformer (TFT)
- **Tip:** Time-Series Forecasting (PyTorch Forecasting)
- **Hedef:** Gelecek fiyat/getiri tahmini.
- **Durum:** HenÃ¼z entegrasyon aÅŸamasÄ±nda. Linux geÃ§iÅŸi ile GPU Ã¼zerinde eÄŸitilebilir hale geldi.
- **Potansiyel:** Volatilite ve rejim deÄŸiÅŸimlerini LSTMs'ten daha iyi yakalayabilir.

---

## 3. Ä°YÄ°LEÅTÄ°RME Ã–NERÄ°LERÄ° VE YOL HARÄ°TASI

### Faz 1: Optimizasyon ve Validasyon (1. Hafta) ğŸ”´ KRÄ°TÄ°K

#### 1.1. Optuna ile Hiperparametre Optimizasyonu
Mevcut statik parametrelerden kurtulup, her eÄŸitim Ã¶ncesi veya periyodik olarak en iyi parametreleri bulan bir script eklenmeli.

**Ã–neri:**
`scripts/optimize_hyperparameters.py` oluÅŸturulacak.

```python
# Taslak Kod Hedefi
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
joblib.dump(study.best_params, "models/saved/optimized_lgbm_params.joblib")
```

#### 1.2. Walk-Forward Validation
Mevcut "son %10 validasyon" yaklaÅŸÄ±mÄ± yerine, zaman iÃ§inde kayan pencerelerle (Rolling Window) modelin kararlÄ±lÄ±ÄŸÄ± test edilmeli.

**Ã–neri:**
`train_models.py` iÃ§inde validasyon mantÄ±ÄŸÄ± gÃ¼ncellenecek. `TimeSeriesSplit` kullanÄ±lacak.

### Faz 2: Backtest ve Risk (2. Hafta) ğŸŸ¡ Ã–NEMLÄ°

#### 2.1. Backtest Stres Testleri
Sistemin 2020 Pandemi dÃ¼ÅŸÃ¼ÅŸÃ¼ veya 2021 Kur Åoku gibi dÃ¶nemlerde nasÄ±l davrandÄ±ÄŸÄ± simÃ¼le edilmeli.

**Ã–neri:**
`StressTester` sÄ±nÄ±fÄ± eklenecek. Belirli tarih aralÄ±klarÄ±nda (Kriz dÃ¶nemleri) backtest Ã§alÄ±ÅŸtÄ±rÄ±p raporlayacak.

#### 2.2. Dinamik Pozisyonlama (Kelly Criterion Ä°yileÅŸtirmesi)
Mevcut Kelly implementasyonu var ancak `risk_manager` ile daha sÄ±kÄ± entegre edilmeli. "Half-Kelly" stratejisi uygulanarak volatilite riskleri dÃ¼ÅŸÃ¼rÃ¼lmeli.

### Faz 3: Feature Engineering (3. Hafta) ğŸŸ¢ OPSÄ°YONEL

#### 3.1. Advanced Features
- **Microstructure Features:** Bid-Ask Spread, Tick Flow (eÄŸer veri varsa).
- **Sentiment Refinement:** KAP haberlerinin sadece sayÄ±sÄ± deÄŸil, iÃ§eriÄŸinin NLP (BERT) ile duygu analizine tabi tutulmasÄ±.

---

## 4. AKSÄ°YON PLANI (Action Items)

AÅŸaÄŸÄ±daki gÃ¶revler `task.md` dosyasÄ±na iÅŸlenerek sÄ±rasÄ±yla uygulanacaktÄ±r.

1.  **[ ] Create Optimization Script:** `scripts/optimize_hyperparameters.py` (Optuna entegrasyonu).
2.  **[ ] Refactor Training Pipeline:** `train_models.py` iÃ§ine optimizasyon adÄ±mÄ±nÄ± opsiyonel olarak ekle.
3.  **[ ] Enhance Backtester:** `run_backtest.py` iÃ§ine Walk-Forward ve Stress Test modlarÄ± ekle.
4.  **[ ] Implement Advanced Config:** Statik eÅŸik deÄŸerlerini (`config.py`) dinamik veya optimize edilebilir hale getir.

---

## 5. KOD ÅABLONLARI (Templates)

### Optuna Objective Fonksiyonu Ã–rneÄŸi

```python
def objective(trial):
    params = {
        'objective': 'lambdarank',
        'metric': 'ndcg',
        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),
        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100)
    }
    
    # ... Train Logic ...
    # return expert_metric
```

### Walk-Forward Loop Ã–rneÄŸi

```python
splits = TimeSeriesSplit(n_splits=5, gap=20)
for train_index, val_index in splits.split(X):
    X_train, X_val = X.iloc[train_index], X.iloc[val_index]
    # ... Train & Eval ...
```
